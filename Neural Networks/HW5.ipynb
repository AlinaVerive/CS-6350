{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec19d4ee-eb50-4445-9cfa-2253f067fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1eaad2-f634-4d99-b8c3-db2f741e4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07adf33f-495c-477f-a991-8540b5f653e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.weights2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.weights3 = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "\n",
    "        # Bias terms\n",
    "        self.bias1 = np.zeros((1, hidden1_size))\n",
    "        self.bias2 = np.zeros((1, hidden2_size))\n",
    "        self.bias3 = np.zeros((1, output_size))\n",
    "\n",
    "        # Learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
    "        self.a3 = sigmoid(self.z3)\n",
    "\n",
    "        return self.a3\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Compute the error\n",
    "        error = output - y\n",
    "\n",
    "        # Backpropagation\n",
    "        d_output = error * sigmoid_derivative(output)\n",
    "        d_weights3 = np.dot(self.a2.T, d_output)\n",
    "\n",
    "        d_hidden2 = np.dot(d_output, self.weights3.T) * sigmoid_derivative(self.a2)\n",
    "        d_weights2 = np.dot(self.a1.T, d_hidden2)\n",
    "\n",
    "        d_hidden1 = np.dot(d_hidden2, self.weights2.T) * sigmoid_derivative(self.a1)\n",
    "        d_weights1 = np.dot(X.T, d_hidden1)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights3 -= self.learning_rate * d_weights3\n",
    "        self.bias3 -= self.learning_rate * np.sum(d_output, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights2 -= self.learning_rate * d_weights2\n",
    "        self.bias2 -= self.learning_rate * np.sum(d_hidden2, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights1 -= self.learning_rate * d_weights1\n",
    "        self.bias1 -= self.learning_rate * np.sum(d_hidden1, axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                loss = np.mean((y - output) ** 2)\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fad95f-1491-4d76-8ad7-ce8c783a1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv(\"train.csv\", header=None)\n",
    "test_data = pd.read_csv(\"test.csv\", header=None)\n",
    "\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f1d23b-6fd4-42a2-a4d9-27a42de3b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f7fb3f7-fc28-43a7-8025-30fba5fa0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "input_size = X_train.shape[1]\n",
    "hidden1_size = 5\n",
    "hidden2_size = 5\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d4086a-bb46-4d83-a3c8-609d0d12fe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.24709158473115123\n",
      "Epoch 200, Loss: 0.2470808007960147\n",
      "Epoch 300, Loss: 0.2469889685692702\n",
      "Epoch 400, Loss: 0.21036139941343485\n",
      "Epoch 500, Loss: 0.012469768135316854\n",
      "Epoch 600, Loss: 0.008819984517570836\n",
      "Epoch 700, Loss: 0.007628763108103337\n",
      "Epoch 800, Loss: 0.007034864947416916\n",
      "Epoch 900, Loss: 0.0066778604317227815\n",
      "Epoch 1000, Loss: 0.006441353794886822\n"
     ]
    }
   ],
   "source": [
    "# Create and train the neural network\n",
    "learning_rate = 0.01\n",
    "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size, output_size, learning_rate)\n",
    "nn.train(X_train, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ca8ad4-5b2d-4a4c-a700-d2e38e4a2e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.986\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "predictions = nn.forward(X_test)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee7b5ac-2512-48e7-a075-b353a39d587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, gamma_0, d):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.weights2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.weights3 = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "\n",
    "        # Bias terms\n",
    "        self.bias1 = np.zeros((1, hidden1_size))\n",
    "        self.bias2 = np.zeros((1, hidden2_size))\n",
    "        self.bias3 = np.zeros((1, output_size))\n",
    "\n",
    "        # Learning rate schedule parameters\n",
    "        self.gamma_0 = gamma_0\n",
    "        self.d = d\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
    "        self.a3 = sigmoid(self.z3)\n",
    "\n",
    "        return self.a3\n",
    "\n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        # Compute the error\n",
    "        error = output - y\n",
    "\n",
    "        # Backpropagation\n",
    "        d_output = error * sigmoid_derivative(output)\n",
    "        d_weights3 = np.dot(self.a2.T, d_output)\n",
    "\n",
    "        d_hidden2 = np.dot(d_output, self.weights3.T) * sigmoid_derivative(self.a2)\n",
    "        d_weights2 = np.dot(self.a1.T, d_hidden2)\n",
    "\n",
    "        d_hidden1 = np.dot(d_hidden2, self.weights2.T) * sigmoid_derivative(self.a1)\n",
    "        d_weights1 = np.dot(X.T, d_hidden1)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights3 -= learning_rate * d_weights3\n",
    "        self.bias3 -= learning_rate * np.sum(d_output, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights2 -= learning_rate * d_weights2\n",
    "        self.bias2 -= learning_rate * np.sum(d_hidden2, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights1 -= learning_rate * d_weights1\n",
    "        self.bias1 -= learning_rate * np.sum(d_hidden1, axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            for t, (x_batch, y_batch) in enumerate(zip(X, y)):\n",
    "                # Calculate the learning rate for the current step\n",
    "                learning_rate = self.gamma_0 / (1 + self.gamma_0 * self.d * t)\n",
    "\n",
    "                # Forward and backward propagation\n",
    "                output = self.forward(x_batch.reshape(1, -1))\n",
    "                self.backward(x_batch.reshape(1, -1), y_batch.reshape(1, -1), output, learning_rate)\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                loss = np.mean((y - self.forward(X)) ** 2)\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72efda08-4a0e-4b66-8973-67a282a9297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize features\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe9ab92-c04b-4f5e-8aa3-d5eee16e8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture and parameters\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 1\n",
    "hidden_layer_sizes = [5, 10, 25, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "254b1b3e-d625-4369-8914-5a74250356a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(hidden_layer_size):\n",
    "    print(f\"\\nTraining with hidden layer size: {hidden_layer_size}\")\n",
    "    gamma_0 = 0.1  # Initial learning rate\n",
    "    d = 0.01  # Decay rate\n",
    "    nn = NeuralNetwork(input_size, hidden_layer_size, hidden_layer_size, output_size, gamma_0, d)\n",
    "    nn.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = nn.forward(X_test)\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f\"Test Accuracy for hidden layer size {hidden_layer_size}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f58952-dbb7-4534-bbbc-a766c8fb9f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with hidden layer size: 5\n",
      "Epoch 100, Loss: 0.006866099258760941\n",
      "Epoch 200, Loss: 0.0007674457242949015\n",
      "Epoch 300, Loss: 0.00032689691887107376\n",
      "Epoch 400, Loss: 0.00020105915952417839\n",
      "Epoch 500, Loss: 0.0001423511558454546\n",
      "Epoch 600, Loss: 0.00010885505048478011\n",
      "Epoch 700, Loss: 8.713363673772276e-05\n",
      "Epoch 800, Loss: 7.219700744921345e-05\n",
      "Epoch 900, Loss: 6.132828243146056e-05\n",
      "Epoch 1000, Loss: 5.303882893383915e-05\n",
      "Test Accuracy for hidden layer size 5: 1.0\n",
      "\n",
      "Training with hidden layer size: 10\n",
      "Epoch 100, Loss: 0.008052095017657506\n",
      "Epoch 200, Loss: 0.001609803145795471\n",
      "Epoch 300, Loss: 0.00045865740349318077\n",
      "Epoch 400, Loss: 0.0002446850400585289\n",
      "Epoch 500, Loss: 0.00016057438434301631\n",
      "Epoch 600, Loss: 0.00011669192712236492\n",
      "Epoch 700, Loss: 9.051740327218446e-05\n",
      "Epoch 800, Loss: 7.318950634337004e-05\n",
      "Epoch 900, Loss: 6.101475319013618e-05\n",
      "Epoch 1000, Loss: 5.20997293577735e-05\n",
      "Test Accuracy for hidden layer size 10: 1.0\n",
      "\n",
      "Training with hidden layer size: 25\n",
      "Epoch 100, Loss: 0.00909202720167871\n",
      "Epoch 200, Loss: 0.004699354652149868\n",
      "Epoch 300, Loss: 0.0013611421613905913\n",
      "Epoch 400, Loss: 0.0005993070629437869\n",
      "Epoch 500, Loss: 0.00034748960102375456\n",
      "Epoch 600, Loss: 0.0002345671623065967\n",
      "Epoch 700, Loss: 0.00017282358752039614\n",
      "Epoch 800, Loss: 0.00013516125838962745\n",
      "Epoch 900, Loss: 0.00011006844100426699\n",
      "Epoch 1000, Loss: 9.222982764153319e-05\n",
      "Test Accuracy for hidden layer size 25: 1.0\n",
      "\n",
      "Training with hidden layer size: 50\n",
      "Epoch 100, Loss: 0.009914993310785202\n",
      "Epoch 200, Loss: 0.007353400656251628\n",
      "Epoch 300, Loss: 0.006526702907500062\n",
      "Epoch 400, Loss: 0.005133600353843168\n",
      "Epoch 500, Loss: 0.001704859231758937\n",
      "Epoch 600, Loss: 0.0006776832547809779\n",
      "Epoch 700, Loss: 0.00037143480731671144\n",
      "Epoch 800, Loss: 0.0002441930224055081\n",
      "Epoch 900, Loss: 0.0001790380213143546\n",
      "Epoch 1000, Loss: 0.00014001498182356902\n",
      "Test Accuracy for hidden layer size 50: 1.0\n",
      "\n",
      "Training with hidden layer size: 100\n",
      "Epoch 100, Loss: 0.011059573326980382\n",
      "Epoch 200, Loss: 0.007952649680075717\n",
      "Epoch 300, Loss: 0.006977281274332045\n",
      "Epoch 400, Loss: 0.006600844876211977\n",
      "Epoch 500, Loss: 0.006231332679657991\n",
      "Epoch 600, Loss: 0.005998594766445679\n",
      "Epoch 700, Loss: 0.004316161355012621\n",
      "Epoch 800, Loss: 0.0016366266739874951\n",
      "Epoch 900, Loss: 0.0007391764367204793\n",
      "Epoch 1000, Loss: 0.0004309092168085576\n",
      "Test Accuracy for hidden layer size 100: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train the network for different hidden layer sizes\n",
    "for hidden_size in hidden_layer_sizes:\n",
    "    train_and_evaluate(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef92ea27-2fec-4a91-aa7a-49386cb9a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (c): Initialize weights to zero and re-train\n",
    "gamma_0 = 0.1  # Initial learning rate\n",
    "d = 0.01  # Decay rate\n",
    "class ZeroWeightNeuralNetwork(NeuralNetwork):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, gamma_0, d):\n",
    "        # Initialize weights and biases to zero\n",
    "        self.weights1 = np.zeros((input_size, hidden1_size))\n",
    "        self.weights2 = np.zeros((hidden1_size, hidden2_size))\n",
    "        self.weights3 = np.zeros((hidden2_size, output_size))\n",
    "\n",
    "        self.bias1 = np.zeros((1, hidden1_size))\n",
    "        self.bias2 = np.zeros((1, hidden2_size))\n",
    "        self.bias3 = np.zeros((1, output_size))\n",
    "\n",
    "        self.gamma_0 = gamma_0\n",
    "        self.d = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50bd063c-1dc3-403e-b8a5-bce8f5776c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with zero-initialized weights\n",
      "Epoch 100, Loss: 0.008180505521284749\n",
      "Epoch 200, Loss: 0.006419787328776942\n",
      "Epoch 300, Loss: 0.006031603766013599\n",
      "Epoch 400, Loss: 0.0059288266630830675\n",
      "Epoch 500, Loss: 0.005854619153580543\n",
      "Epoch 600, Loss: 0.0057878404758818\n",
      "Epoch 700, Loss: 0.005927932017731422\n",
      "Epoch 800, Loss: 0.005814363531782429\n",
      "Epoch 900, Loss: 0.005762574344481722\n",
      "Epoch 1000, Loss: 0.005750199908237601\n"
     ]
    }
   ],
   "source": [
    "# Train with zero-initialized weights\n",
    "print(\"\\nTraining with zero-initialized weights\")\n",
    "nn_zero = ZeroWeightNeuralNetwork(input_size, 10, 10, output_size, gamma_0, d)\n",
    "nn_zero.train(X_train, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db4665da-e867-4037-b5af-f70932e80394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with zero-initialized weights: 0.992\n"
     ]
    }
   ],
   "source": [
    "# Test the model with zero-initialized weights\n",
    "predictions = nn_zero.forward(X_test)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy with zero-initialized weights: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb895203-ee5f-4579-8358-e89d94bae224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
