{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f2fbfb-929e-468c-818a-af136e451464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"train.csv\", header=None)\n",
    "test_data = pd.read_csv(\"test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da3a56d-9c47-4426-a925-425c7529aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to {1, -1}\n",
    "train_data[4] = train_data[4].apply(lambda x: 1 if x == 1 else -1)\n",
    "test_data[4] = test_data[4].apply(lambda x: 1 if x == 1 else -1)\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1].values\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7e3a78-8fba-40d5-83c7-9c8ada2a0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM with stochastic sub-gradient descent\n",
    "def svm_sgd(X, y, C, gamma_schedule, max_epochs=100):\n",
    "    \"\"\"\n",
    "    SVM using stochastic sub-gradient descent in the primal domain.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Features (numpy array)\n",
    "    y: Labels (numpy array)\n",
    "    C: Regularization parameter\n",
    "    gamma_schedule: Function to calculate learning rate gamma_t\n",
    "    max_epochs: Maximum number of epochs\n",
    "    \n",
    "    Returns:\n",
    "    weights, biases: Model parameters\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Shuffle the training data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X, y = X[indices], y[indices]\n",
    "        \n",
    "        for t, (x_i, y_i) in enumerate(zip(X, y)):\n",
    "            gamma_t = gamma_schedule(epoch * n_samples + t)\n",
    "            condition = y_i * (np.dot(w, x_i) + b) < 1\n",
    "            \n",
    "            if condition:\n",
    "                w = (1 - gamma_t) * w + gamma_t * C * y_i * x_i\n",
    "                b = b + gamma_t * C * y_i\n",
    "            else:\n",
    "                w = (1 - gamma_t) * w\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32315170-90db-4d70-8481-925129854cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedules\n",
    "def gamma_schedule_a(t, gamma_0=0.1, a=0.01):\n",
    "    return gamma_0 / (1 + gamma_0 * a * t)\n",
    "\n",
    "def gamma_schedule_b(t, gamma_0=0.1):\n",
    "    return gamma_0 / (1 + t)\n",
    "\n",
    "# Training and testing functions\n",
    "def compute_error(X, y, w, b):\n",
    "    predictions = np.sign(np.dot(X, w) + b)\n",
    "    return np.mean(predictions != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac0f16c-93bf-4806-8892-2f03efc683c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "C_values = [100 / 873, 500 / 873, 700 / 873]\n",
    "gamma_0 = 0.1\n",
    "a = 0.01\n",
    "\n",
    "# Training with both schedules\n",
    "results = {}\n",
    "for C in C_values:\n",
    "    # Schedule (a)\n",
    "    w_a, b_a = svm_sgd(X_train, y_train, C, lambda t: gamma_schedule_a(t, gamma_0, a))\n",
    "    train_error_a = compute_error(X_train, y_train, w_a, b_a)\n",
    "    test_error_a = compute_error(X_test, y_test, w_a, b_a)\n",
    "    \n",
    "    # Schedule (b)\n",
    "    w_b, b_b = svm_sgd(X_train, y_train, C, lambda t: gamma_schedule_b(t, gamma_0))\n",
    "    train_error_b = compute_error(X_train, y_train, w_b, b_b)\n",
    "    test_error_b = compute_error(X_test, y_test, w_b, b_b)\n",
    "    \n",
    "    # Store results\n",
    "    results[C] = {\n",
    "        'Schedule_a': {'train_error': train_error_a, 'test_error': test_error_a},\n",
    "        'Schedule_b': {'train_error': train_error_b, 'test_error': test_error_b},\n",
    "        'weights_diff': np.linalg.norm(w_a - w_b),\n",
    "        'bias_diff': np.abs(b_a - b_b)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f2f5fd-bc23-4e28-bb62-4ada36cd2a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Schedule_a  \\\n",
      "0.114548  {'train_error': 0.05963302752293578, 'test_err...   \n",
      "0.572738  {'train_error': 0.022935779816513763, 'test_er...   \n",
      "0.801833  {'train_error': 0.020642201834862386, 'test_er...   \n",
      "\n",
      "                                                 Schedule_b weights_diff  \\\n",
      "0.114548  {'train_error': 0.18577981651376146, 'test_err...     0.045637   \n",
      "0.572738  {'train_error': 0.0481651376146789, 'test_erro...     0.131222   \n",
      "0.801833  {'train_error': 0.027522935779816515, 'test_er...     0.080821   \n",
      "\n",
      "         bias_diff  \n",
      "0.114548  0.143152  \n",
      "0.572738   0.50323  \n",
      "0.801833  0.408985  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817938f-b4e1-4bf6-8092-7cded980aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedule a generally has lower training and testing errors than Schedule b, indicating it may converge more effectively for this task.\n",
    "# Testing errors are slightly higher than training errors, which is expected due to generalization.\n",
    "# These weights and bias differences seem to decrease as C increases, \n",
    "# indicating that higher C values might lead to more similar models between the schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c51da2-280e-48d5-8a3e-e47b26d7718a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alinav.AWS.000\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:437: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  fx = wrapped_fun(x)\n",
      "C:\\Users\\alinav.AWS.000\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:441: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  g = append(wrapped_grad(x), 0.0)\n",
      "C:\\Users\\alinav.AWS.000\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:495: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  a_eq = vstack([con['jac'](x, *con['args'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0092\n",
      "Test Error: 0.0100\n",
      "Weights: [-0.94292626 -0.6514918  -0.73372188 -0.04102192]\n",
      "Bias: 1.0745032328893676\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Dual SVM implementation with optimization\n",
    "def dual_svm_linear_optimized(X, y, C):\n",
    "    \"\"\"\n",
    "    Optimized Dual SVM implementation for linear kernel.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Compute the kernel matrix (linear kernel)\n",
    "    K = np.dot(X, X.T)\n",
    "    \n",
    "    # Define the objective function\n",
    "    P = np.outer(y, y) * K  # Precompute for vectorized form\n",
    "    def objective(alpha):\n",
    "        return 0.5 * alpha @ P @ alpha - np.sum(alpha)\n",
    "    \n",
    "    # Equality constraint: sum(alpha * y) = 0\n",
    "    constraints = {'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)}\n",
    "    \n",
    "    # Bounds: 0 <= alpha_i <= C\n",
    "    bounds = [(0, C) for _ in range(n_samples)]\n",
    "    \n",
    "    # Initial guess\n",
    "    alpha_0 = np.zeros(n_samples)\n",
    "    \n",
    "    # Solve the optimization problem using SLSQP\n",
    "    result = minimize(\n",
    "        objective, alpha_0, method='SLSQP', bounds=bounds, constraints=constraints,\n",
    "        options={'maxiter': 100, 'ftol': 1e-6, 'disp': False}\n",
    "    )\n",
    "    \n",
    "    return result.x  # Lagrange multipliers (alpha)\n",
    "\n",
    "# Function to recover weights and bias\n",
    "def recover_weights_bias(alpha, X, y):\n",
    "    \"\"\"\n",
    "    Recover weights and bias from dual SVM.\n",
    "    \"\"\"\n",
    "    w = np.sum((alpha * y)[:, None] * X, axis=0)\n",
    "    support_vectors = alpha > 1e-5\n",
    "    b = np.mean(y[support_vectors] - np.dot(X[support_vectors], w))\n",
    "    return w, b\n",
    "\n",
    "# Test with one value of C for development\n",
    "C = 100 / 873\n",
    "\n",
    "# Train the dual SVM\n",
    "alpha = dual_svm_linear_optimized(X_train, y_train, C)\n",
    "\n",
    "# Recover weights and bias\n",
    "w, b = recover_weights_bias(alpha, X_train, y_train)\n",
    "\n",
    "# Compute training and test errors\n",
    "train_error = compute_error(X_train, y_train, w, b)\n",
    "test_error = compute_error(X_test, y_test, w, b)\n",
    "\n",
    "# Display results\n",
    "print(f\"Train Error: {train_error:.4f}\")\n",
    "print(f\"Test Error: {test_error:.4f}\")\n",
    "print(f\"Weights: {w}\")\n",
    "print(f\"Bias: {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97caaab5-8a02-4c19-8b5b-d4f27e20e0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Train Error  Test Error  Support Vectors\n",
      "0.114548 0.1       0.000000       0.002            869.0\n",
      "         0.5       0.000000       0.002            825.0\n",
      "         1.0       0.000000       0.002            805.0\n",
      "         5.0       0.008028       0.006            442.0\n",
      "         100.0     0.003440       0.004            290.0\n",
      "0.572738 0.1       0.000000       0.002            869.0\n",
      "         0.5       0.000000       0.002            731.0\n",
      "         1.0       0.000000       0.002            556.0\n",
      "         5.0       0.000000       0.002            208.0\n",
      "         100.0     0.000000       0.000            116.0\n",
      "0.801833 0.1       0.000000       0.002            868.0\n",
      "         0.5       0.000000       0.002            694.0\n",
      "         1.0       0.000000       0.002            528.0\n",
      "         5.0       0.000000       0.002            194.0\n",
      "         100.0     0.000000       0.000             99.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "\n",
    "# Gaussian kernel function\n",
    "def gaussian_kernel(X, gamma):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian kernel matrix for a dataset X.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i, j] = np.exp(-np.linalg.norm(X[i] - X[j])**2 / gamma)\n",
    "    return K\n",
    "\n",
    "# Dual SVM with Gaussian kernel\n",
    "def dual_svm_gaussian(X, y, C, gamma):\n",
    "    \"\"\"\n",
    "    Dual SVM implementation for Gaussian kernel using scipy.optimize.minimize.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = gaussian_kernel(X, gamma)\n",
    "    \n",
    "    # Define the objective function\n",
    "    P = np.outer(y, y) * K\n",
    "    def objective(alpha):\n",
    "        return 0.5 * alpha @ P @ alpha - np.sum(alpha)\n",
    "    \n",
    "    # Equality constraint: sum(alpha * y) = 0\n",
    "    constraints = {'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)}\n",
    "    \n",
    "    # Bounds: 0 <= alpha_i <= C\n",
    "    bounds = [(0, C) for _ in range(n_samples)]\n",
    "    \n",
    "    # Initial guess\n",
    "    alpha_0 = np.zeros(n_samples)\n",
    "    \n",
    "    result = minimize(\n",
    "        objective, alpha_0, method='SLSQP', bounds=bounds, constraints=constraints,\n",
    "        options={'maxiter': 100, 'ftol': 1e-6, 'disp': False}\n",
    "    )\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "# Prediction function for Gaussian kernel\n",
    "def predict_gaussian(alpha, X_train, y_train, X_test, gamma):\n",
    "    \"\"\"\n",
    "    Predict labels for test data using the dual SVM with Gaussian kernel.\n",
    "    \"\"\"\n",
    "    n_samples = X_test.shape[0]\n",
    "    K = np.zeros((n_samples, X_train.shape[0]))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            K[i, j] = np.exp(-np.linalg.norm(X_test[i] - X_train[j])**2 / gamma)\n",
    "    \n",
    "    # Ensure correct broadcasting\n",
    "    weighted_sum = np.sum((alpha * y_train)[None, :] * K, axis=1)\n",
    "    return np.sign(weighted_sum)\n",
    "\n",
    "# Hyperparameters\n",
    "C_values = [100 / 873, 500 / 873, 700 / 873]\n",
    "gamma_values = [0.1, 0.5, 1, 5, 100]\n",
    "\n",
    "# Store results\n",
    "gaussian_results = {}\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        # Train the dual SVM\n",
    "        alpha = dual_svm_gaussian(X_train, y_train, C, gamma)\n",
    "        \n",
    "        # Predict on training and test data\n",
    "        train_predictions = predict_gaussian(alpha, X_train, y_train, X_train, gamma)\n",
    "        test_predictions = predict_gaussian(alpha, X_train, y_train, X_test, gamma)\n",
    "        \n",
    "        # Compute errors\n",
    "        train_error = np.mean(train_predictions != y_train)\n",
    "        test_error = np.mean(test_predictions != y_test)\n",
    "        \n",
    "        # Count support vectors\n",
    "        support_vectors = np.sum(alpha > 1e-5)\n",
    "        \n",
    "        # Store results\n",
    "        gaussian_results[(C, gamma)] = {\n",
    "            'train_error': train_error,\n",
    "            'test_error': test_error,\n",
    "            'support_vectors': support_vectors\n",
    "        }\n",
    "\n",
    "gaussian_results_df = pd.DataFrame(gaussian_results).T\n",
    "gaussian_results_df.columns = ['Train Error', 'Test Error', 'Support Vectors']\n",
    "\n",
    "print(gaussian_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f4709-135a-456d-8049-e75571034ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
